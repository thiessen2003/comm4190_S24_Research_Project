# Spiral of Silence and the Development of Biased LLMs

## Introduction

Large Language Models (LLMs) have revolutionized the field of natural language processing, enabling the generation of human-like text and facilitating various applications such as content creation, virtual assistants, and chatbots. However, the reliance on web-scraped data for training these models has raised concerns about the potential biases and inaccuracies present in the training data. As LLMs learn from vast amounts of online content, they may inadvertently incorporate societal biases and prejudices, leading to biased or discriminatory outputs. This project explores the relationship between the spiral of silence theory and the development of biased LLMs, investigating how the dynamics of public opinion formation and the suppression of minority views can amplify biases in these models.


## Motivation

The motivation behind this project stems from the growing concern about the impact of biased LLMs on society. As these models become increasingly integrated into various domains, from social media and content generation to education and policy-making, it is crucial to understand and address the potential harms caused by the generation of biased or discriminatory outputs. Also, the spiral of silence theory provides a valuable framework for examining how the fear of social isolation and the desire to conform to perceived majority opinions can suppress diverse perspectives and reinforce existing power structures and inequalities. Thus, it felt logical and reasonable to intersect these two areas in the research. 

## Goals

- To explore the relationship between the spiral of silence theory and the development of biased LLMs.
- To investigate the role of web-scraped training data in perpetuating societal biases and prejudices in LLMs and to develop strategies for mitigating these biases.
- To promote transparency and interpretability in LLMs, enabling users to understand how these models arrive at their outputs and to identify potential sources of bias.
- To test and prove the actual existence of bias and discriminatory tendencies on the most popular LLMs nowadays. 
- To promote collaboration between researchers, developers, and stakeholders from diverse backgrounds, encouraging the development of inclusive and ethically-aware AI systems that prioritize social good and fairness.

## Methodology

The methodology used to develop this project can be divided into bibliographical research and practical LLM testing. Regarding the bibliographical research, the research focused on exploring diverse concepts related to AI, LLMs, and the spiral of silence through the lens of previous academic work and making inferences and connections between the fields. Also, to further prove the statements and points raised in the bibliographical research part of the paper, a series of experiments were conducted to evaluate the presence of bias and discriminatory tendencies in modern LLMs. 

## Results

Even though this research is, at its essence, exploratory, I hope that the conclusions drawn from this research motivate technology developers to be more careful and concerned about the impacts that LLMs can cause when not appropriately maintained and managed. I also seek to raise awareness that LLMs, despite their multiple benefits, can be problematic if not used cautiously. 

## File Structure and Contents

- **[Bibliography](./Bibliography):** Contains all cited works and literature used throughout the research.
- **[Relevant_pdfs](./Relevant_pdfs):** Includes pdfs of relevant articles and papers.
- **[Relevant_Images](./Relevant_Images):** Stores images used in the project documentation and analysis.
- **[Testing_Biased_Views](./Testing_Biased_Views):** Contains test files and results of bias assessments in LLMs.
- **[Research.ipynb](./Research.ipynb):** Jupyter notebook with the research itself.
- **[README.md](./README.md):** This file explains the project's purpose, structure, and navigation.


